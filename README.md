# CLIPGraphs

This repository contains the code for obtaining the results for [CLIPGraphs: Multimodal graph networks to infer object-room affinities for scene rearrangement](https://clipgraphs.github.io)

To reproduce the results corresponding to CLIP ViT-H/14 model:
```
python get_results.py --split test
python get_results.py --split val
```

This will print the statistics for the model with different metrics and generate a new file called `GCN_model_output.txt` containing the predicted object-room mappings.


To get the predicted object-room mappings for different language baselines, run the script:
```
python llm_baseline.py --lang_model glove
```
Here, you can replace `glove` with any of the following language models: `roberta`, `glove`, `clip_convnext_base`, `clip_RN50`, or `clip_ViT-H/14`.

This would produce 2 files, `lang_model_mAP.txt` contains the statistic metrics for the metric, and `lang_model_output.txt` which contains the object-room mappings generated by the lang_model.