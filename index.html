<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Multimodal commonsense reasoning for Scene Rarrangement ">
    <meta name="keywords" content="CLIPGraphs, Embodied AI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Housekeep</title>


<!--     <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> -->

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    Brojeshwar Bhowmick2,
Krishna Murthy Jatavallabhula4, Mohan Sridharan3, Madhava Krishna

    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"> CLIPGraphs: Multimodal graph networks to infer object-room affinities for scene rearrangement </h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
              <a href="https://ayush8120.github.io/">Ayush Agrawal</a><sup>1</sup>*,</span>
                            <span class="author-block">
              <a href="https://arunram.me/">Raghav Arora</a><sup>1</sup>*,</span>
                            <span class="author-block">
              <a href="https://yvsriram.github.io">Ahana Datta</a><sup>1</sup>,
            </span>
                            <span class="author-block">
              <a href="https://www.gilitschenski.org/">Snehasis Banerjee</a><sup>1,2</sup>,
            </span>
                            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~dbatra/">Brojeshwar Bhowmick</a><sup>2</sup>,
            </span>
                            <span class="author-block">
              <a href="https://www.andrewszot.com/">Krishna Murthy Jatavallabhula</a><sup>4</sup>*,
            </span>
                            <span class="author-block">
              <a href="https://dexter1691.github.io/">Mohan Sridharan</a><sup>3</sup>*
            </span>
                            <span class="author-block">
              <a href="https://dexter1691.github.io/">Madhava Krishna</a><sup>1</sup>*
            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>Robotics Research Center, IIIT Hyderabad, India</span>
                            <span class="author-block"><sup>2</sup>TCS Research, Tata Consultancy Services, India</span>
                            <span class="author-block"><sup>3</sup>Intelligent Robotics Lab, University of Birmingham, UK</span><br>
                            <span class="author-block"><sup>3</sup>MIT CSAIL</span><br>
                            <span class="author-block">*Equal Contribution</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <span class="link-block">
                <a href="https://arxiv.org/pdf/2205.10712.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                                <span>Paper</span>
                                </a>
                                </span>
                                <span class="link-block">
                <a href="https://arxiv.org/abs/2205.10712"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                                <span>arXiv</span>
                                </a>
                                </span>
                                <!-- Video Link. -->
                                <span class="link-block">
                <a href="https://www.youtube.com/channel/UCVrTOJ5spohegXQ6jTpfY8A"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                                <span>Video</span>
                                </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                <a href="https://github.com/yashkant/housekeep"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                                <span>Code</span>
                                </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                <a href="https://drive.google.com/drive/folders/1pnFWwEAtSupY0MCmvfYYaf6-kD9c0dp8"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                                <span>Data</span>
                                </a>
                                </span>
                                <!-- Colab Notebook Link. -->
                                <span class="link-block">
                <a href="https://colab.research.google.com/drive/12LweZ0xlGlO_SP4-pz1wbb5tOlXHLZ5R?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-code"></i>
                  </span>
                                <span>Colab</span>
                                </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <center>
                    <video id="teaser" autoplay muted loop playsinline width="100%">
                    <source src="./static/images/teaser_video.mp4"
                        type="video/mp4">
                    </video>
                    <!-- img src='static/images/teaser_figure.png' -->
                </center>
                <h2 class="subtitle has-text-centered">
                    We introduce <span class="dnerf">Housekeep</span>, a benchmark to evaluate commonsense reasoning in the home for embodied AI.
                </h2>
            </div>
        </div>
    </section>



    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content">
                        <p>
                           We propose a multimodal learning approach to infer human preferences of a <it>tidy room</it>.
This is crucial for robotic scene rearrangement problems, where much of the prior work has centered around assuming rearrangement goals to be concretely specified.
                        </p>
                        <p>Our method, dubbed CLIPGraphs, encodes features from off-the-shelf vision-language models (specifically CLIP) using a graph neural network over the set of rooms and objects present in a scene.
CLIPGraphs are multimodal - they may be queries using both vision and language queries, and map each query to a room of the house the object should belong to, in line with human preferences.
                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Dataset</h2>
                    <div class="content">
                        <p> The <span class="dnerf">Housekeep</span> dataset includes 1799 object models and 395 receptacle models from five popular asset repositories â€“ Amazon Berkeley Objects (AB), Google Scanned Objects (GSO), ReplicaCAD (R-CAD), iGibson
                            and YCB Objects.
                        </p>
                    </div>
                    <div class="content">
                        <img src="static/images/high_level_category.png"> This table gives the number of object and receptacle models obtained from different sources.
                    </div>
                    <h2 class="subtitle has-text-centered">
                    </h2>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{kant2022housekeep,
            title={Housekeep: Tidying Virtual Households using Commonsense Reasoning}, 
            author={Yash Kant and Arun Ramachandran and Sriram Yenamandra and Igor Gilitschenski and Dhruv Batra and Andrew Szot and Harsh Agrawal},
            year={2022},
            eprint={2205.10712},
            archivePrefix={arXiv},
            primaryClass={cs.CV}}</code></pre>
        </div>
    </section>


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://arxiv.org/abs/2205.10712">
                    <i class="fas fa-file-pdf"></i>
                </a>
                <a class="icon-link" href="https://github.com/yashkant" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p align="center">
                            The template for this website was borrowed from <a href="https://nerfies.github.io/">https://nerfies.github.io/</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>
